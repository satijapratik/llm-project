{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (BitsAndBytesConfig, TrainingArguments)\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "import tqdm\n",
    "# from trl import SFTTrainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_train = pd.read_json(\"dataset/training_data.json\")  # Use `lines=True` for line-delimited JSON\n",
    "data_train = data_train.transpose()\n",
    "data_test = pd.read_json(\"dataset/testing_data.json\")\n",
    "data_test = data_test.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.astype(str)\n",
    "data_test = data_test.astype(str)\n",
    "train_dataset = Dataset.from_pandas(data_train)\n",
    "test_dataset = Dataset.from_pandas(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d23153de83a45b98011b90a7e45b2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "base_model= AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=bnb_config,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "new_model = \"llama_fine_tuned\"\n",
    "\n",
    "train_dataset = train_dataset.train_test_split(test_size=0.1)\n",
    "rank = 16\n",
    "alpha = rank*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'closest_5_queries', '__index_level_0__'],\n",
       "        num_rows: 36027\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', 'closest_5_queries', '__index_level_0__'],\n",
       "        num_rows: 4003\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_input_and_output(data):\n",
    "#     result = {}\n",
    "#     for key, value in data.items():\n",
    "#         print(key)\n",
    "#         print(value)\n",
    "#         input_text = value[\"input\"]\n",
    "#         raw_output = value[\"output\"][0][\"raw_output\"]  # Assuming only one item in 'output'\n",
    "#         result[input_text] = raw_output\n",
    "#     return result\n",
    "\n",
    "# # Extracted dictionary\n",
    "# extracted_data = extract_input_and_output(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=rank,\n",
    "    lora_alpha=rank,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "# target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "\n",
    "def train(\n",
    "    model: PeftModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    train_dataset: Dataset,\n",
    "    grad_acc_steps: int = 1,\n",
    "    batch_size: int = 32,\n",
    "    epochs: int = 1,\n",
    ") -> None:\n",
    "    tokenized_dataset = train_dataset.map(\n",
    "        lambda x: {\n",
    "            \"input_ids\": tokenizer.encode(x[\"input\"] + x[\"output\"])\n",
    "            + [tokenizer.eos_token_id]\n",
    "        }\n",
    "    )\n",
    "    # .remove_columns([\"text\", \"target\", \"label\"])\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "        shuffle=True,\n",
    "    )\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=4e-4)\n",
    "    step = 0\n",
    "    for epoch_num in range(epochs):\n",
    "        for batch in (pbar := tqdm(dataloader, desc=f\"epoch {epoch_num+1}/{epochs}\")):\n",
    "            batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            outputs[\"loss\"].backward()\n",
    "            print(\"Loss: \", outputs[\"loss\"])\n",
    "\n",
    "            if (step + 1) % grad_acc_steps == 0:\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "\n",
    "            pbar.set_postfix({\"loss\": outputs[\"loss\"].item()})\n",
    "            step += 1\n",
    "\n",
    "    model.save_pretrained(\"llama3.2-1B-quantized-lora\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27098bfd220e4e2db125457da366a65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36027 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d20981e6474219a5faec67f4f2308a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model, tokenizer, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_arguments = TrainingArguments(\n",
    "#     output_dir=new_model,\n",
    "#     per_device_train_batch_size=6,\n",
    "#     per_device_eval_batch_size=6,\n",
    "#     log_level=\"debug\",\n",
    "#     gradient_accumulation_steps=6,\n",
    "#     optim=\"paged_adamw_32bit\",\n",
    "#     num_train_epochs=3,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=6,\n",
    "#     logging_steps=6,\n",
    "#     save_steps=6,\n",
    "#     warmup_steps=10,\n",
    "#     logging_strategy=\"steps\",\n",
    "#     learning_rate=3e-4,\n",
    "#     warmup_ratio=0.1,\n",
    "#     load_best_model_at_end=True,\n",
    "#     overwrite_output_dir=True,\n",
    "#     fp16=True,\n",
    "#     bf16=False,\n",
    "#     group_by_length=True,\n",
    "#     lr_scheduler_type=\"linear\",\n",
    "#     report_to=\"none\",\n",
    "# )\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model=base_model,\n",
    "#     train_dataset=train_dataset[\"train\"],\n",
    "#     eval_dataset=train_dataset[\"test\"],\n",
    "#     peft_config=peft_config,\n",
    "#     max_seq_length=3000,\n",
    "#     dataset_text_field=\"texts\",\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_arguments,\n",
    "#     packing= False,\n",
    "# )\n",
    "\n",
    "# initial_eval_values = trainer.evaluate()\n",
    "# print(initial_eval_values)\n",
    "# initial_eval_loss = initial_eval_values['eval_loss']\n",
    "# trainer.train()\n",
    "\n",
    "# training_loss_history = []\n",
    "# eval_loss_history = [initial_eval_loss]\n",
    "# for step in trainer.state.log_history:\n",
    "#   if 'loss' in step:\n",
    "#     training_loss_history.append(step['loss'])\n",
    "#   elif \"eval_loss\" in step:\n",
    "#     eval_loss_history.append(step['eval_loss'])\n",
    "\n",
    "# print(training_loss_history)\n",
    "# print(eval_loss_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmu-llms-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
